{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOn1cnCKQT6d"
   },
   "source": [
    "# Instructions of this exercise\n",
    "\n",
    "This jupyter notebook is an exercise for you to understand the basic word2vec skipgram model, as well as other foundations in NLP tasks, such as corpus preprocessing, model training and inference (with pytorch).\n",
    "\n",
    "The different sections have comments that will guide you through the different stages of the processing.\n",
    "\n",
    "**You will find parts in this notebook that are missing. They are marked with a <font color=\"red\">\"TODO\"</font>, with an explanation about what you are supposed to write/code in that gap. Sometimes you may be requested to implement an algorithm or a piece of code that behaves in a specific way. Other times, you may be requested to explain what a specific piece of code does or why it is needed.** In some cases, the requests may contain a \"trap\", e.g. the thing you are requested to implement can't be done for whatever reason, or it may need an extra preprocessing step for it to be possible.\n",
    "\n",
    "If you have any doubt, do not hesitate to contact the instructor at noe.casas@upc.edu\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifhifRKlyotX"
   },
   "source": [
    "# Requirements\n",
    "\n",
    "In order to follow this notebook you need working knowledge on:\n",
    "\n",
    "* Python programming.\n",
    "* Pytorch basics.\n",
    "* Word embedding basics.\n",
    "* Unix command line basics (cat, head, tail, awk).\n",
    "\n",
    "If you lack any of them, please acquire the needed knowledge on the internet, where there is plenty of material about the three topics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDyNSZSRXQ52"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this jupyter notebook, we will illustrate how the word2vec **skipgram** model\n",
    "works. We will be using the 2 word2vec original articles as guidelines to\n",
    "implement a simplified version of it:\n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality)\n",
    "\n",
    "Word2vec is an approach to create word embeddings. It proposes two variants:\n",
    "* Continuous Bag of Words (CBoW)\n",
    "* Skipgram\n",
    "\n",
    "In both cases, word2vec proposes to use a shallow neural network trained on\n",
    "textual data. After the training, we simply take the resulting word embeddings\n",
    "and use them in the downstream task of our choice.\n",
    "\n",
    "## Word2vec Algorithms\n",
    "\n",
    "The **Continuous Bag of Words (CBoW)** word2vec variant proposes to train a\n",
    "neural net to predict a word taking as input its surrounding words up to a certain distance. This way, assuming a maximum distance of 2, for each word $w_t$ in our training text (where $t$ is the position of the word), we will create 1 training sample where the expected output is $w_t$ and the input is $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$.\n",
    "\n",
    "The **Skipgram** word2vec variant proposes to train a neural net to predict the surrounding words (up to a certain distance) taking as input the word itself. This way, assuming a maximum distance of 2, for each word $w_t$ in our training text, we will create 4 training samples where the input is $w_t$ and the expected outputs are each of $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$.\n",
    "\n",
    "The skipgram model can be applied extra elements that improve the quality of the embeddings. **These improvements ARE NOT USED IN THIS NOTEBOOK**. Nevertheless -just to satisfy your curiosity- they are:\n",
    "* Resampling based on distance to the focus word. See [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) section 3.2:\n",
    "> Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.\n",
    "* Hierarchical softmax. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.1.\n",
    "* Negative sampling. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.2.\n",
    "* Subsampling of Frequent Words. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfDUPXJHuJOk"
   },
   "source": [
    "## Hardware Resources\n",
    "\n",
    "Let's have a look at the GPU we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "7-YrCqinuJ4K",
    "outputId": "b581a0c1-2e67-411a-ade2-2760342c546e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  2 10:17:47 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6KYn0X5XjjJ"
   },
   "source": [
    "---\n",
    "## Installation of extra libraries\n",
    "\n",
    "**NOTE**: In jupyter, apart from being able to execute Python code, you can also run commands in the command line by preceding the invocation with a \"`!`\". For instance, if you want to execute \"`ls`\", you may simply write `!ls`\n",
    "\n",
    "While Google Colab has a lot of libraries installed by default, we need some extra ones. In order to install them, we will use `pip`, executing it from the command line with `!`.\n",
    "\n",
    "The extra libraries we will use are:\n",
    "* **sacremoses**: the is a classical translation tool called \"Moses\" (https://github.com/moses-smt/mosesdecoder). Apart from translation tools, it offers a lot of different preprocessing scripts (written in the Perl programming language). These scripts are ubiquitous in the NLP world. Instead of using Moses scripts, we will use sacremoses, which is a reimplementation of the most useful Moses scripts in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "A9nO7Iu-FKup",
    "outputId": "836d9a46-f90c-49f2-d0b8-f4c910a907e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/alvations/sacremoses/archive/master.zip\n",
      "\u001b[?25l  Downloading https://github.com/alvations/sacremoses/archive/master.zip\n",
      "\u001b[K     / 1.9MB 1.2MB/s\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.38) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.38) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.38) (7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.38) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.38) (4.38.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=883294 sha256=11d3d6bd4ef5ad8ff533ea9750894ecb17ed65871fe51adcccad3e5451f4cb40\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h0o9ntno/wheels/a9/8b/db/e688b9d5eecd3518dd27712980c76599bcd8312ee220f4878e\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.0.38\n"
     ]
    }
   ],
   "source": [
    "!pip install -U https://github.com/alvations/sacremoses/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs5xmv2NdoRV"
   },
   "source": [
    "---\n",
    "\n",
    "## General Imports\n",
    "\n",
    "Now we will import some components that will be used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TKrQWEltqVI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeLR98bJuV8O"
   },
   "source": [
    "---\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "We fix the random seed for reproducibility purposes. See this for details:\n",
    "https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oN_B25-ctw2t"
   },
   "outputs": [],
   "source": [
    "seed = 321\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lm01kM-Td4El"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Download\n",
    "\n",
    "For this notebook we will use a dataset widely used in machine translation, called \"News Commentary\". We will be using some English subsets of News Commentary as training, validation and test data.\n",
    "\n",
    "First, we will download the dataset from the internet and extract some parts into files `train.en`, `valid.en` and `test.en`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ5lGJO9t55o"
   },
   "outputs": [],
   "source": [
    "!wget --quiet 'http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz'\n",
    "!tar xzf training-parallel-nc-v12.tgz\n",
    "!mv training/news-commentary-v12.fr-en.en ./train.en\n",
    "!cat training/news-commentary-v12.zh-en.en | shuf --random-source=<(openssl enc -aes-256-ctr -pass pass:42 -nosalt </dev/zero 2>/dev/null) > shuffled.en\n",
    "!head -5000 < shuffled.en > valid.en\n",
    "!tail -5000 < shuffled.en > test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLBlhh0ceaIX"
   },
   "source": [
    "News commentary is text from news, where each line of the file is a separate sentence. Let's take a look at the number of sentences in each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "WurMWf7jvDKR",
    "outputId": "321b14ea-09f4-488d-d619-119299f686dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  258432 train.en\n",
      "    5000 valid.en\n",
      "    5000 test.en\n",
      "  268432 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l train.en valid.en test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTftXikMemRy"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "In general, it is of utmost importance to understand the data you are working with, so let's have a look at the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "EO4icbYMEjgv",
    "outputId": "82e5bd87-6727-4550-d2b7-2865f98ae613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$10,000 Gold?\n",
      "SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.\n",
      "Lately, with gold prices up more than 300% over the last decade, it is harder than ever.\n",
      "Just last December, fellow economists Martin Feldstein and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment, sensibly pointing out gold’s risks.\n",
      "Wouldn’t you know it?\n"
     ]
    }
   ],
   "source": [
    "!head -5 train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MJ0fZQzeyg5"
   },
   "source": [
    "As a first preprocessing step, we will:\n",
    "* **normalize punctuation**: convert punctuation marks so that only the most standard ones are used. For instance, unicode curly quotes (“”) will be converted to straigh ones (\").\n",
    "* **tokenize**: words and punctuation will be separated with spaces, and contractions may be expanded. For instance, sentence \"No, he wasn't there.\" will be converted to \"No , he was n't there .\"\n",
    "* **lowercase**: turn all alphabetical characters into lowercase, so that the embeddings we obtain are independent from the original casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "yeDdDO7WFH7X",
    "outputId": "4b856d6a-a0a2-420b-f10d-fd9b789c8988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 258961/258961 [00:27<00:00, 9381.65it/s]\n",
      "100% 258961/258961 [00:40<00:00, 6375.85it/s]\n",
      "100% 5000/5000 [00:00<00:00, 8543.98it/s]\n",
      "100% 5000/5000 [00:00<00:00, 6799.14it/s]\n",
      "100% 5004/5004 [00:00<00:00, 11341.59it/s]\n",
      "100% 5004/5004 [00:00<00:00, 5618.10it/s]\n"
     ]
    }
   ],
   "source": [
    "!cat train.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > train.low.tok.en\n",
    "!cat valid.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > valid.low.tok.en\n",
    "!cat test.en  | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > test.low.tok.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FAWv2wcgoY0"
   },
   "source": [
    "Let's look how our data looks like after the described preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "yVI139fcLZov",
    "outputId": "8ad94d0a-771e-4491-f541-fdd1281093cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 10,000 gold ?\n",
      "san francisco - it has never been easy to have a rational conversation about the value of gold .\n",
      "lately , with gold prices up more than 300 % over the last decade , it is harder than ever .\n",
      "just last december , fellow economists martin feldstein and nouriel roubini each penned op-eds bravely questioning bullish market sentiment , sensibly pointing out gold &apos;s risks .\n",
      "wouldn &apos;t you know it ?\n",
      "since their articles appeared , the price of gold has moved up still further . gold prices even hit a record-high $ 1,300 recently .\n",
      "last december , many gold bugs were arguing that the price was inevitably headed for $ 2,000 .\n",
      "now , emboldened by continuing appreciation , some are suggesting that gold could be headed even higher than that .\n",
      "one successful gold investor recently explained to me that stock prices languished for a more than a decade before the dow jones index crossed the 1,000 mark in the early 1980 &apos;s .\n",
      "since then , the index has climbed above 10,000 .\n"
     ]
    }
   ],
   "source": [
    "!head -10 train.low.tok.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "di9XfAPBjNr_"
   },
   "source": [
    "Now we will **mask the numbers** in the text with a \"NUM\" placeholder. We do this because we don't want to create embeddings of the numbers present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fX3piGFYLszp"
   },
   "outputs": [],
   "source": [
    "!cat train.low.tok.en | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > train.low.tok.nonum.en\n",
    "!cat valid.low.tok.en | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > valid.low.tok.nonum.en\n",
    "!cat test.low.tok.en  | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > test.low.tok.nonum.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "mX4v4dN1Op2Z",
    "outputId": "a3a6a697-9508-4501-ac04-2fac30ac5c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ NUM gold ?\n",
      "san francisco - it has never been easy to have a rational conversation about the value of gold .\n",
      "lately , with gold prices up more than NUM % over the last decade , it is harder than ever .\n",
      "just last december , fellow economists martin feldstein and nouriel roubini each penned op-eds bravely questioning bullish market sentiment , sensibly pointing out gold &apos;s risks .\n",
      "wouldn &apos;t you know it ?\n",
      "since their articles appeared , the price of gold has moved up still further . gold prices even hit a record-high $ NUM recently .\n",
      "last december , many gold bugs were arguing that the price was inevitably headed for $ NUM .\n",
      "now , emboldened by continuing appreciation , some are suggesting that gold could be headed even higher than that .\n",
      "one successful gold investor recently explained to me that stock prices languished for a more than a decade before the dow jones index crossed the NUM mark in the early NUM &apos;s .\n",
      "since then , the index has climbed above NUM .\n"
     ]
    }
   ],
   "source": [
    "!head -10 train.low.tok.nonum.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PzvmUV-jaku"
   },
   "source": [
    "Finally, we will **remove the punctuation** from the text, as we just want to compute word embeddings, nor including commas, stops or quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myIUEyi4OB7Z"
   },
   "outputs": [],
   "source": [
    "!cat train.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\\+, ,g' > train.low.tok.nonum.nopunct.en\n",
    "!cat valid.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\\+, ,g' > valid.low.tok.nonum.nopunct.en\n",
    "!cat test.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\\+, ,g' > test.low.tok.nonum.nopunct.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "QhY993DDOuuN",
    "outputId": "360dd2dc-6c57-442b-a5fa-1ab0ccedd397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NUM gold \n",
      "san francisco it has never been easy to have a rational conversation about the value of gold \n",
      "lately with gold prices up more than NUM over the last decade it is harder than ever \n",
      "just last december fellow economists martin feldstein and nouriel roubini each penned opeds bravely questioning bullish market sentiment sensibly pointing out gold aposs risks \n",
      "wouldn apost you know it \n",
      "since their articles appeared the price of gold has moved up still further gold prices even hit a recordhigh NUM recently \n",
      "last december many gold bugs were arguing that the price was inevitably headed for NUM \n",
      "now emboldened by continuing appreciation some are suggesting that gold could be headed even higher than that \n",
      "one successful gold investor recently explained to me that stock prices languished for a more than a decade before the dow jones index crossed the NUM mark in the early NUM aposs \n",
      "since then the index has climbed above NUM \n"
     ]
    }
   ],
   "source": [
    "!head -10 train.low.tok.nonum.nopunct.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZOTS2VPjzxL"
   },
   "source": [
    "Now we have our 3 final files: `train.low.tok.nonum.nopunct.en`, `valid.low.tok.nonum.nopunct.en` and `test.low.tok.nonum.nopunct.en`.\n",
    "\n",
    "Note that, at each preprocessing step, we have been creating new files with extra suffixes indicating the type of preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yKXradWjvyi"
   },
   "source": [
    "---\n",
    "\n",
    "## Text Loading\n",
    "\n",
    "We will load the text from our 3 files into python variables. The text in a file will be represented as a list of sentences, where each sentence is a list of words (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f31ITJwM40Sf"
   },
   "outputs": [],
   "source": [
    "def load_text_tokens(filename):\n",
    "    \"\"\"\n",
    "    Loads the text in the filename as a list of list of words.\n",
    "    :param filename Name of the file to load in memory.\n",
    "    :return List of list of strings with the contents of the file.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip() # remove leading and training blanks\n",
    "\n",
    "            # split words at blanks (the text is already tokeniked)\n",
    "            line_tokens = line.split(' ')\n",
    "\n",
    "            if len(line_tokens) < 5:\n",
    "              continue  # ignore too short lines\n",
    "\n",
    "            text.append(line_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnSzCiFTUWV2"
   },
   "outputs": [],
   "source": [
    "train_tokens = load_text_tokens('train.low.tok.nonum.nopunct.en')\n",
    "valid_tokens = load_text_tokens('valid.low.tok.nonum.nopunct.en')\n",
    "test_tokens = load_text_tokens('test.low.tok.nonum.nopunct.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ha5EHhoAlEfd"
   },
   "source": [
    "## Vocabulary Extraction\n",
    "\n",
    "Given the huge amount of possible word in a language, when creating word embeddings, we need to constrain the set of supported words. This way, we will be having a list of words for which we will compute embeddings. This list is known as the **vocabulary**. The vocabulary is created by selecting the most frequent words in the training data.\n",
    "\n",
    "As the vocabulary is finite, there are some words it won't be able to represent. We will use a **special token** to represent these \"out of vocabulary\" words, normally referred to as the **<UNK>** token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDr3uo_YjrC9"
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(text_tokens, vocab_size):\n",
    "    from collections import Counter\n",
    "    unk_token = '<UNK>'\n",
    "    special_tokens = [unk_token]\n",
    "    counter = Counter(word for sentence in text_tokens for word in sentence)\n",
    "    most_common_counts = counter.most_common(vocab_size - len(special_tokens))\n",
    "    most_frequent_words = [word for (word, count) in most_common_counts]\n",
    "    idx2token = special_tokens + most_frequent_words\n",
    "    token2idx = {token: token_id for token_id, token in enumerate(idx2token)}\n",
    "    return unk_token, idx2token, token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXsDVis4jtWf"
   },
   "outputs": [],
   "source": [
    "unk_token, idx2token, token2idx = extract_vocabulary(train_tokens, vocab_size=50000)\n",
    "unk_token_id = token2idx[unk_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2gLL3fGlKHx"
   },
   "source": [
    "# Text Encoding\n",
    "\n",
    "To feed words to our neural network, we need to express them as numbers. This way, a word will be represented as the index it occupies in the vocabulary table. With this in mind, let's encoder the training, validation and test datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxRk85UNkpk_"
   },
   "outputs": [],
   "source": [
    "def encode_token_ids(text_tokens, vocabulary, unk_token_id):\n",
    "    return [[vocabulary.get(token, unk_token_id) for token in sentence]\n",
    "            for sentence in text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AUC2-Wiks7N"
   },
   "outputs": [],
   "source": [
    "train_token_ids = encode_token_ids(train_tokens, token2idx, unk_token_id)\n",
    "valid_token_ids = encode_token_ids(test_tokens, token2idx, unk_token_id)\n",
    "test_token_ids = encode_token_ids(test_tokens, token2idx, unk_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hcVQlCClWu9"
   },
   "source": [
    "---\n",
    "\n",
    "## Preparation of the data for the skipgram model\n",
    "\n",
    "\n",
    "Now we will prepare the dataset according to the skipgram approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4HCbsErMXeT"
   },
   "outputs": [],
   "source": [
    "def create_skipgram_dataset(token_ids, unk_token_id, window):\n",
    "    \"\"\"\n",
    "\n",
    "    :param token_ids: token IDs to be adapted to the skipgram model. It is \n",
    "                      a list of lists of token IDs (integers).\n",
    "    :param unk_token_id token ID of the <UNK> token.\n",
    "    :param window distance in words to the left/right that the context will include.\n",
    "    :return A list of tuples (input_token_id, output_token_id). No tuples with\n",
    "            the unk_token_id are returned.\n",
    "    \"\"\"\n",
    "    data = []\n",
    " \n",
    "    # TODO: implement the code to get the training data (inputs and\n",
    "        #       expected outputs) according to the skipgram model. You can\n",
    "        #       review the basics of skipgrams in the introduction above,\n",
    "        #       and you can also check the assertions below to better understand\n",
    "        #       what this function is supposed to receive and generate as\n",
    "        #       output.\n",
    "        \n",
    "    for sentence_token_ids in token_ids: #recorre la list de token_ids (en los ejemplos es de dimensión 1)\n",
    "        length=len(sentence_token_ids)\n",
    "        for index, words in enumerate(sentence_token_ids): #recorre la lista de números de cada sentece_token_id\n",
    "            \n",
    "            if (index - window)>=0 and (index + window)<length and words!=unk_token_id:\n",
    "                \n",
    "                for x in range(-window, 0):\n",
    "                    \n",
    "                    if (sentence_token_ids[index+x]!=unk_token_id):\n",
    "                        data.append((words, sentence_token_ids[index+x]))\n",
    "                    \n",
    "                for x in range(1, window+1):\n",
    "                   \n",
    "                    if (sentence_token_ids[index+x]!=unk_token_id):\n",
    "                        data.append((words, sentence_token_ids[index+x]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQK8EmtXOzyw"
   },
   "source": [
    "These \"unit tests\" may help to 1) understand what it is expected from the function and 2) validate the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcZfnucOOOH_"
   },
   "outputs": [],
   "source": [
    "assert create_skipgram_dataset([[1, 2, 3, 4, 5]], 0, 2) == [(3, 1), (3, 2), (3, 4), (3, 5)]\n",
    "assert create_skipgram_dataset([[1, 2, 3, 4, 5, 6]], 0, 2) == [(3, 1), (3, 2), (3, 4), (3, 5), (4, 2), (4, 3), (4, 5), (4, 6)]\n",
    "assert create_skipgram_dataset([[1, 2, 3, 0, 5, 6]], 0, 2) == [(3, 1), (3, 2), (3, 5)]\n",
    "assert create_skipgram_dataset([[1, 0, 3, 4, 5]], 0, 2) == [(3, 1), (3, 4), (3, 5)]\n",
    "assert create_skipgram_dataset([[1, 0, 3, 4, 5, 0, 7]], 0, 3) == [(4, 1), (4, 3), (4, 5), (4, 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDR3FEDgPJtf"
   },
   "source": [
    "Now, let's prepare the actual datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXZlIEJilVKf"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "skipgram_train = create_skipgram_dataset(train_token_ids, unk_token_id, window=window_size)\n",
    "skipgram_valid = create_skipgram_dataset(valid_token_ids, unk_token_id, window=window_size)\n",
    "skipgram_test = create_skipgram_dataset(test_token_ids, unk_token_id, window=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6y1dQMZnRMw"
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Now we start with the Pytorch stuff. First, the model. Actually, it's the model together with the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_w9olu27kGwH"
   },
   "outputs": [],
   "source": [
    "class SkipgramLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Network that implements the basic Skipgram word2vec, without the negative\n",
    "    sampling loss or hierarchical softmax.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param vocab_size Size of the vocabulary.\n",
    "        :param embedding_size Size of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, output):\n",
    "        \"\"\"\n",
    "        :param input LongTensor with shape [batch_size, input_size]\n",
    "        :param output LongTensor with shape [batch_size]\n",
    "        \"\"\"\n",
    "        v_wi = self.embedding(input)    # [bsz, embsz]\n",
    "        v_all = self.embedding.weight   # [vocab_size, embsz]\n",
    "\n",
    "        # TODO: implement the computation of the logits of the softmax according\n",
    "        #       to equation (2) in the article \"Distributed Representations of\n",
    "        #       Words and Phrases and their Compositionality\".\n",
    "        #       (http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality)\n",
    "\n",
    "        logits = torch.mm(v_wi,v_all.T)\n",
    "        return self.cross_entropy(logits,output)\n",
    "    \n",
    "    def get_embedding_table(self):\n",
    "        return self.embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QKExxvySdfT"
   },
   "source": [
    "Now, let's define a function that trains the model for a whole epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPnjtkv1585v"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_data_loader, loss_function, optimizer, log_period, epoch):\n",
    "  \"\"\"\n",
    "  Trains the given model for 1 epoch.\n",
    "  :param train_data_loader Data loader of the training data. Batches must be\n",
    "                           a tuple of input and output data.\n",
    "  :param loss_function Loss function to train.\n",
    "  :param optimizer Optimizer object to use.\n",
    "  :param log_period Number of steps after which to print a trace.\n",
    "  :param epoch Epoch to print in the traces.\n",
    "  :return List of losses for every training step.\n",
    "  \"\"\"\n",
    "  loss_function.train()\n",
    "\n",
    "  num_total_data = len(train_data_loader.dataset)\n",
    "  num_processed_data = 0\n",
    "\n",
    "  losses = []\n",
    "  for batch_ndx, batch in enumerate(train_data_loader):\n",
    "      input, output = batch\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_function(input.cuda(), output.cuda())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_value = loss.item()\n",
    "      losses.append(loss_value)\n",
    "      num_processed_data += batch[0].size(0)\n",
    "\n",
    "      if log_period and batch_ndx % log_period == 0:\n",
    "          percentage = 100 * num_processed_data // num_total_data\n",
    "          avg_loss = loss_value if batch_ndx == 0 else np.mean(losses[-log_period + 1:])\n",
    "          msg = \"epoch: {}, progress: {}%, avg.loss: {:.3f}\"\n",
    "          print(msg.format(epoch, percentage, avg_loss))\n",
    "  \n",
    "  return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1s_tVAiSi7e"
   },
   "source": [
    "Now, a function to evaluate our model in the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNkj7FHznhtH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval_loss(data_loader, loss_function):\n",
    "    \"\"\"\n",
    "    Computes the loss function over the given data in inference mode.\n",
    "    :param data_loader Data loader over which to evaluate loss_function.\n",
    "    :param loss_function Loss function to evaluate.\n",
    "    :return Value (float) of the loss.\n",
    "    \"\"\"\n",
    "    loss_function.eval()\n",
    "    \n",
    "    num_total_data = len(data_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            input, output = batch\n",
    "            bsz = input.size(0)\n",
    "            batch_loss = loss_function(input.cuda(), output.cuda())\n",
    "            \n",
    "            # we average over all batches\n",
    "            loss += bsz * batch_loss.item() / num_total_data\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFZ6tnnESn0f"
   },
   "source": [
    "Finally, a function to create and train the embeddings for N epochs, evaluating the model every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlAREP6J6Ihd"
   },
   "outputs": [],
   "source": [
    "def train_embedding(train_data,\n",
    "                    valid_data,\n",
    "                    vocab_size,\n",
    "                    emb_size,\n",
    "                    num_epochs,\n",
    "                    batch_size,\n",
    "                    log_period=None):\n",
    "  \"\"\"\n",
    "  Trains embeddings with the given parameters.\n",
    "  :param train_data Training data (list of tuples of input and output token IDs).\n",
    "  :param valid_data Validation data (list of tuples of input and output token IDs).\n",
    "  :param vocab_size Size of the vocabulary.\n",
    "  :param emd_size Dimensionality of the embedding.\n",
    "  :param num_epochs Number of epochs to train.\n",
    "  :param batch_size Batch size to use.\n",
    "  :param log_period Number of steps after which to print a trace.\n",
    "  :return A tupe with the embedding tensor, the list of the training\n",
    "          losses and the list of the validation losses.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the whole training a validation data as pytorch tensors\n",
    "  train_input = torch.from_numpy(np.array([input for (input, __) in train_data]))\n",
    "  train_output = torch.from_numpy(np.array([output for (_, output) in train_data]))\n",
    "  valid_input = torch.from_numpy(np.array([input for (input, _) in valid_data]))\n",
    "  valid_output = torch.from_numpy(np.array([output for (_, output) in valid_data]))\n",
    "\n",
    "  # Define data loaders for the training and validation data\n",
    "  train_loader = DataLoader(TensorDataset(train_input, train_output),\n",
    "                            batch_size=batch_size,\n",
    "                            pin_memory=True)\n",
    "  valid_loader = DataLoader(TensorDataset(valid_input, valid_output),\n",
    "                            batch_size=batch_size,\n",
    "                            pin_memory=True)\n",
    "\n",
    "  # Create the model (+ loss function) and move it to the GPU\n",
    "  loss_function = SkipgramLoss(vocab_size, emb_size).cuda()\n",
    "\n",
    "  # Define the optimizer and LR scheduler\n",
    "  optimizer = optim.Adam(lr=0.005, params=loss_function.parameters())\n",
    "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "  # Define lists to collect the training and validation losses in case\n",
    "  # we want to plot them later.\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "\n",
    "  # Iterate the requested number of epochs\n",
    "  for epoch in range(num_epochs):\n",
    "      # Train for one epoch\n",
    "      train_losses.extend(train_epoch(train_loader,\n",
    "                                      loss_function,\n",
    "                                      optimizer,\n",
    "                                      log_period,\n",
    "                                      epoch))\n",
    "\n",
    "      # Evaluate the model on the validation data\n",
    "      valid_loss = eval_loss(valid_loader, loss_function)\n",
    "      valid_losses.append(valid_loss)\n",
    "      \n",
    "      scheduler.step()\n",
    "\n",
    "      if log_period:\n",
    "          # Print traces every once in a while\n",
    "          print(\"epoch: {}, validation loss: {:.3f}\".format(epoch, valid_loss))\n",
    "  \n",
    "  # This is the embedding table that we have been training\n",
    "  embedding = loss_function.get_embedding_table()\n",
    "\n",
    "  return embedding, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ihe9DEVLfLRo"
   },
   "source": [
    "And now we actually train the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "colab_type": "code",
    "id": "EUAvnF5oBPT5",
    "outputId": "38ff8be1-05dd-4dd1-ecc3-46d0c530d01d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, progress: 0%, avg.loss: 64.786\n",
      "epoch: 0, progress: 8%, avg.loss: 35.255\n",
      "epoch: 0, progress: 16%, avg.loss: 16.143\n",
      "epoch: 0, progress: 25%, avg.loss: 12.199\n",
      "epoch: 0, progress: 33%, avg.loss: 9.967\n",
      "epoch: 0, progress: 42%, avg.loss: 9.164\n",
      "epoch: 0, progress: 50%, avg.loss: 8.651\n",
      "epoch: 0, progress: 59%, avg.loss: 8.339\n",
      "epoch: 0, progress: 67%, avg.loss: 8.044\n",
      "epoch: 0, progress: 76%, avg.loss: 7.840\n",
      "epoch: 0, progress: 84%, avg.loss: 7.670\n",
      "epoch: 0, progress: 93%, avg.loss: 7.659\n",
      "epoch: 0, validation loss: 7.481\n",
      "epoch: 1, progress: 0%, avg.loss: 8.185\n",
      "epoch: 1, progress: 8%, avg.loss: 7.916\n",
      "epoch: 1, progress: 16%, avg.loss: 7.400\n",
      "epoch: 1, progress: 25%, avg.loss: 7.466\n",
      "epoch: 1, progress: 33%, avg.loss: 7.296\n",
      "epoch: 1, progress: 42%, avg.loss: 7.350\n",
      "epoch: 1, progress: 50%, avg.loss: 7.356\n",
      "epoch: 1, progress: 59%, avg.loss: 7.357\n",
      "epoch: 1, progress: 67%, avg.loss: 7.324\n",
      "epoch: 1, progress: 76%, avg.loss: 7.286\n",
      "epoch: 1, progress: 84%, avg.loss: 7.247\n",
      "epoch: 1, progress: 93%, avg.loss: 7.267\n",
      "epoch: 1, validation loss: 7.320\n",
      "epoch: 2, progress: 0%, avg.loss: 7.948\n",
      "epoch: 2, progress: 8%, avg.loss: 7.741\n",
      "epoch: 2, progress: 16%, avg.loss: 7.307\n",
      "epoch: 2, progress: 25%, avg.loss: 7.373\n",
      "epoch: 2, progress: 33%, avg.loss: 7.226\n",
      "epoch: 2, progress: 42%, avg.loss: 7.282\n",
      "epoch: 2, progress: 50%, avg.loss: 7.291\n",
      "epoch: 2, progress: 59%, avg.loss: 7.299\n",
      "epoch: 2, progress: 67%, avg.loss: 7.272\n",
      "epoch: 2, progress: 76%, avg.loss: 7.237\n",
      "epoch: 2, progress: 84%, avg.loss: 7.206\n",
      "epoch: 2, progress: 93%, avg.loss: 7.222\n",
      "epoch: 2, validation loss: 7.307\n"
     ]
    }
   ],
   "source": [
    "embedding, train_losses, valid_losses = train_embedding(skipgram_train,\n",
    "                                                        skipgram_valid,\n",
    "                                                        vocab_size=len(idx2token),\n",
    "                                                        emb_size=64,\n",
    "                                                        num_epochs=3,\n",
    "                                                        batch_size=4000,\n",
    "                                                        log_period=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucjW4SMdfUaw"
   },
   "source": [
    "---\n",
    "\n",
    "## Explore the Embedded Vectors\n",
    "\n",
    "First, let's define a function to find the list of K closes vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x6JK8oiK82D"
   },
   "outputs": [],
   "source": [
    "def find_closest_ids(embedding, vector, k):\n",
    "    \"\"\"\n",
    "    Finds the closes K vectors to the given one among the given embedding matrix.\n",
    "    :param embedding Embedding matrix.\n",
    "    :param vector Vector to which we should find the closes ones in the matrix.\n",
    "    :param k Number of closest elements to find.\n",
    "    \"\"\"\n",
    "    similarity = F.cosine_similarity(embedding, vector.unsqueeze(0))\n",
    "    index_sorted = torch.argsort(similarity).cpu().numpy()\n",
    "    top_k = index_sorted[-k:]\n",
    "    return list(reversed(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mbq1zbv2n9u-"
   },
   "source": [
    "Let's find the words that are close to some target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "vXYPfCPw_Oll",
    "outputId": "407491df-eb1d-4c68-a710-48a9ecef0ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to minister: ['prime', 'former', 'british', 'david', 'deputy']\n",
      "Similar words to france: ['germany', 'italy', 'britain', 'russia', 'japan']\n",
      "Similar words to measures: ['policies', 'implement', 'such', 'reforms', 'austerity']\n"
     ]
    }
   ],
   "source": [
    "words = [\"minister\", \"france\", \"measures\"]\n",
    "\n",
    "for word in words:\n",
    "    #TODO: print the closes 5 words to each of the words in the list\n",
    "    word = word.lower()\n",
    "    idx = token2idx.get(word) #token2idx[word] would raise an error if word is not present in the dictionary\n",
    "    if idx is None: idx = token2idx.get('<UNK>')\n",
    "    vector= embedding[idx]\n",
    "\n",
    "    similar_token_ids = find_closest_ids(embedding, vector, 6)\n",
    "    similar_words = [idx2token[idx] for idx in similar_token_ids]\n",
    "    print(\"Similar words to {}: {}\".format(word, similar_words[1:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85lmvdhFoC24"
   },
   "source": [
    "The \"geometry\" of word embedded spaces has been widely studied, observing the  \"parallelogram\" formed by the vectors of two pairs of words with analogous relationships. You can read more about this in this recent ICML article: http://proceedings.mlr.press/v97/allen19a.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "54F469OhFg06",
    "outputId": "d5fb2b88-6088-4dc4-ee42-149396c3bba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris is to france, as berlin is to germany\n",
      "france\n"
     ]
    }
   ],
   "source": [
    "# TODO: making use of the function find_closest_ids, try to find two pairs of\n",
    "#       words that verify the parallelogram in embedding space and two pairs of\n",
    "#       words that do not verify it.\n",
    "\n",
    "\n",
    "#The following examples work well:\n",
    "words = [\"paris\", \"france\", \"berlin\"]      #Expected result: \"germany\"  (country/capital)\n",
    "#words = [\"play\", \"plays\", \"have\"]          #Expected result: \"has\"      (First person/Third person)\n",
    "\n",
    "#The following examples work with second most similar word:\n",
    "#words = [\"big\", \"bigger\", \"large\"]         #Expected result: \"larger\"   (superlative)\n",
    "#words = [\"day\", \"night\", \"good\"]           #Expected result: \"bad\"      (Opposition)\n",
    "#words = [\"spain\", \"spanish\", \"italy\"]      #Expected result: \"italian\"  (Demonym)\n",
    "\n",
    "#The following examples don't work:\n",
    "#words=[\"britain\", \"pound\", \"germany\"]       #Expected result: \"euro\"    (country/coin)\n",
    "#words = [\"play\", \"played\", \"write\"]         #Expected result: \"wrote\"   (Present/Past)\n",
    "\n",
    "\n",
    "idx = [token2idx.get(word) for word in words]\n",
    "vectors = embedding[idx]\n",
    "vector_estimate = vectors[1] - vectors[0] + vectors[2]\n",
    "\n",
    "similar_token_ids = find_closest_ids(embedding, vector_estimate, 2)\n",
    "similar_words = idx2token[similar_token_ids[0]]\n",
    "print(\"{} is to {}, as {} is to {}\".format(words[0], words[1], words[2], similar_words))\n",
    "print(idx2token[similar_token_ids[1]]) #This is the second most similar word, which happens to be the expected one with the examples of the second group above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-W-8cOgzfNzr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEO6-3yFoJDC"
   },
   "source": [
    "Nevertheless, be aware that many of the parallelogram examples in the literature (specially those that were considered to show gender bias) where due to an implementation detail in gensim, which explicitly avoided the returned vector to be the same as the original one. Read about this at https://twitter.com/rikvannoord/status/1132933236756238341\n",
    "\n",
    "\n",
    "**TODO**: Original codes had a condition which none of the inputs could be returned as an output. For instance, if the input was (man: doctor, woman:?), the input couldn't be none of (man, doctor, woman), so it returned nurse. It seemed a question of bias but if the input had been woman: doctor, man:?), the output would probably be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_dU4ZqDCnR_"
   },
   "source": [
    "---\n",
    "\n",
    "This is not to say that our data, models or resulting word embeddings are bias free. On the contrary, there is **gender bias** in the three of them. Check the literature on gender bias in word embeddings to know more. You can start with these two seminal works:\n",
    "* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings)\n",
    "* [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SLPDL_notebook1_Team2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
