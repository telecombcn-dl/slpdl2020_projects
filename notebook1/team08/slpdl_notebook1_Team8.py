# -*- coding: utf-8 -*-
"""SLPDL_notebook1_Lluis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1du-hZ2YGPI5zA_WvPhhzbpufo_1Wc2E6

# Instructions of this exercise

This jupyter notebook is an exercise for you to understand the basic word2vec skipgram model, as well as other foundations in NLP tasks, such as corpus preprocessing, model training and inference (with pytorch).

The different sections have comments that will guide you through the different stages of the processing.

**You will find parts in this notebook that are missing. They are marked with a <font color="red">"TODO"</font>, with an explanation about what you are supposed to write/code in that gap. Sometimes you may be requested to implement an algorithm or a piece of code that behaves in a specific way. Other times, you may be requested to explain what a specific piece of code does or why it is needed.** In some cases, the requests may contain a "trap", e.g. the thing you are requested to implement can't be done for whatever reason, or it may need an extra preprocessing step for it to be possible.

If you have any doubt, do not hesitate to contact the instructor at noe.casas@upc.edu

---

# Requirements

In order to follow this notebook you need working knowledge on:

* Python programming.
* Pytorch basics.
* Word embedding basics.
* Unix command line basics (cat, head, tail, awk).

If you lack any of them, please acquire the needed knowledge on the internet, where there is plenty of material about the three topics.

---

# Introduction

In this jupyter notebook, we will illustrate how the word2vec **skipgram** model
works. We will be using the 2 word2vec original articles as guidelines to
implement a simplified version of it:

* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
* [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality)

Word2vec is an approach to create word embeddings. It proposes two variants:
* Continuous Bag of Words (CBoW)
* Skipgram

In both cases, word2vec proposes to use a shallow neural network trained on
textual data. After the training, we simply take the resulting word embeddings
and use them in the downstream task of our choice.

## Word2vec Algorithms

The **Continuous Bag of Words (CBoW)** word2vec variant proposes to train a
neural net to predict a word taking as input its surrounding words up to a certain distance. This way, assuming a maximum distance of 2, for each word $w_t$ in our training text (where $t$ is the position of the word), we will create 1 training sample where the expected output is $w_t$ and the input is $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$.

The **Skipgram** word2vec variant proposes to train a neural net to predict the surrounding words (up to a certain distance) taking as input the word itself. This way, assuming a maximum distance of 2, for each word $w_t$ in our training text, we will create 4 training samples where the input is $w_t$ and the expected outputs are each of $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$.

The skipgram model can be applied extra elements that improve the quality of the embeddings. **These improvements ARE NOT USED IN THIS NOTEBOOK**. Nevertheless -just to satisfy your curiosity- they are:
* Resampling based on distance to the focus word. See [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) section 3.2:
> Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.
* Hierarchical softmax. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.1.
* Negative sampling. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.2.
* Subsampling of Frequent Words. See [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) section 2.3.

## Hardware Resources

Let's have a look at the GPU we will be using:
"""

!nvidia-smi

"""---
## Installation of extra libraries

**NOTE**: In jupyter, apart from being able to execute Python code, you can also run commands in the command line by preceding the invocation with a "`!`". For instance, if you want to execute "`ls`", you may simply write `!ls`

While Google Colab has a lot of libraries installed by default, we need some extra ones. In order to install them, we will use `pip`, executing it from the command line with `!`.

The extra libraries we will use are:
* **sacremoses**: the is a classical translation tool called "Moses" (https://github.com/moses-smt/mosesdecoder). Apart from translation tools, it offers a lot of different preprocessing scripts (written in the Perl programming language). These scripts are ubiquitous in the NLP world. Instead of using Moses scripts, we will use sacremoses, which is a reimplementation of the most useful Moses scripts in Python.
"""

!pip install -U https://github.com/alvations/sacremoses/archive/master.zip

"""---

## General Imports

Now we will import some components that will be used throughout the notebook
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

"""---

## Reproducibility

We fix the random seed for reproducibility purposes. See this for details:
https://pytorch.org/docs/stable/notes/randomness.html
"""

seed = 321
torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(seed)

"""---

## Data Download

For this notebook we will use a dataset widely used in machine translation, called "News Commentary". We will be using some English subsets of News Commentary as training, validation and test data.

First, we will download the dataset from the internet and extract some parts into files `train.en`, `valid.en` and `test.en`:
"""

!wget --quiet 'http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz'
!tar xzf training-parallel-nc-v12.tgz
!mv training/news-commentary-v12.fr-en.en ./train.en
!cat training/news-commentary-v12.zh-en.en | shuf --random-source=<(openssl enc -aes-256-ctr -pass pass:42 -nosalt </dev/zero 2>/dev/null) > shuffled.en
!head -5000 < shuffled.en > valid.en
!tail -5000 < shuffled.en > test.en

"""News commentary is text from news, where each line of the file is a separate sentence. Let's take a look at the number of sentences in each file:"""

!wc -l train.en valid.en test.en

"""---


## Data preprocessing

In general, it is of utmost importance to understand the data you are working with, so let's have a look at the text:
"""

!head -5 train.en

"""As a first preprocessing step, we will:
* **normalize punctuation**: convert punctuation marks so that only the most standard ones are used. For instance, unicode curly quotes (“”) will be converted to straigh ones (").
* **tokenize**: words and punctuation will be separated with spaces, and contractions may be expanded. For instance, sentence "No, he wasn't there." will be converted to "No , he was n't there ."
* **lowercase**: turn all alphabetical characters into lowercase, so that the embeddings we obtain are independent from the original casing.
"""

!cat train.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > train.low.tok.en
!cat valid.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > valid.low.tok.en
!cat test.en  | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > test.low.tok.en

"""Let's look how our data looks like after the described preprocessing steps:"""

!head -10 train.low.tok.en

"""Now we will **mask the numbers** in the text with a "NUM" placeholder. We do this because we don't want to create embeddings of the numbers present in the text."""

!cat train.low.tok.en | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > train.low.tok.nonum.en
!cat valid.low.tok.en | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > valid.low.tok.nonum.en
!cat test.low.tok.en  | sed 's/[0-9]*[.,0-9]*[0-9]/NUM/g' > test.low.tok.nonum.en

!head -10 train.low.tok.nonum.en

"""Finally, we will **remove the punctuation** from the text, as we just want to compute word embeddings, nor including commas, stops or quotes."""

!cat train.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\+, ,g' > train.low.tok.nonum.nopunct.en
!cat valid.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\+, ,g' > valid.low.tok.nonum.nopunct.en
!cat test.low.tok.nonum.en | tr -d '[:punct:]' | sed 's,[ ]\+, ,g' > test.low.tok.nonum.nopunct.en

!head -10 train.low.tok.nonum.nopunct.en

"""Now we have our 3 final files: `train.low.tok.nonum.nopunct.en`, `valid.low.tok.nonum.nopunct.en` and `test.low.tok.nonum.nopunct.en`.

Note that, at each preprocessing step, we have been creating new files with extra suffixes indicating the type of preprocessing.

---

## Text Loading

We will load the text from our 3 files into python variables. The text in a file will be represented as a list of sentences, where each sentence is a list of words (strings).
"""

def load_text_tokens(filename):
    """
    Loads the text in the filename as a list of list of words.
    :param filename Name of the file to load in memory.
    :return List of list of strings with the contents of the file.
    """
    text = []
    with open(filename) as f:
        for line in f:
            line = line.strip() # remove leading and training blanks

            # split words at blanks (the text is already tokeniked)
            line_tokens = line.split(' ')

            if len(line_tokens) < 5:
              continue  # ignore too short lines

            text.append(line_tokens)
    return text

train_tokens = load_text_tokens('train.low.tok.nonum.nopunct.en')
valid_tokens = load_text_tokens('valid.low.tok.nonum.nopunct.en')
test_tokens = load_text_tokens('test.low.tok.nonum.nopunct.en')

"""## Vocabulary Extraction

Given the huge amount of possible word in a language, when creating word embeddings, we need to constrain the set of supported words. This way, we will be having a list of words for which we will compute embeddings. This list is known as the **vocabulary**. The vocabulary is created by selecting the most frequent words in the training data.

As the vocabulary is finite, there are some words it won't be able to represent. We will use a **special token** to represent these "out of vocabulary" words, normally referred to as the **<UNK>** token.
"""

def extract_vocabulary(text_tokens, vocab_size):
    from collections import Counter
    unk_token = '<UNK>'
    special_tokens = [unk_token]
    counter = Counter(word for sentence in text_tokens for word in sentence)
    most_common_counts = counter.most_common(vocab_size - len(special_tokens))
    most_frequent_words = [word for (word, count) in most_common_counts]
    idx2token = special_tokens + most_frequent_words
    token2idx = {token: token_id for token_id, token in enumerate(idx2token)}
    return unk_token, idx2token, token2idx

unk_token, idx2token, token2idx = extract_vocabulary(train_tokens, vocab_size=50000)
unk_token_id = token2idx[unk_token]

"""# Text Encoding

To feed words to our neural network, we need to express them as numbers. This way, a word will be represented as the index it occupies in the vocabulary table. With this in mind, let's encoder the training, validation and test datasets:
"""

def encode_token_ids(text_tokens, vocabulary, unk_token_id):
    return [[vocabulary.get(token, unk_token_id) for token in sentence]
            for sentence in text_tokens]

train_token_ids = encode_token_ids(train_tokens, token2idx, unk_token_id)
valid_token_ids = encode_token_ids(test_tokens, token2idx, unk_token_id)
test_token_ids = encode_token_ids(test_tokens, token2idx, unk_token_id)

"""---

## Preparation of the data for the skipgram model


Now we will prepare the dataset according to the skipgram approach:
"""

def create_skipgram_dataset(token_ids, unk_token_id, window):
    """

    :param token_ids: token IDs to be adapted to the skipgram model. It is 
                      a list of lists of token IDs (integers).
    :param unk_token_id token ID of the <UNK> token.
    :param window distance in words to the left/right that the context will include.
    :return A list of tuples (input_token_id, output_token_id). No tuples with
            the unk_token_id are returned.
    """
    data = []
    for sentence_token_ids in token_ids:
        # TODO: implement the code to get the training data (inputs and
        #       expected outputs) according to the skipgram model. You can
        #       review the basics of skipgrams in the introduction above,
        #       and you can also check the assertions below to better understand
        #       what this function is supposed to receive and generate as
        #       output.
        for ind, token in enumerate(sentence_token_ids):
          if len(sentence_token_ids)-1-ind >= window and ind >= window: #check it is within the adecuated range
            if token!=unk_token_id: #check it's not an unknown
              for pair in range(ind-window, ind+window+1): #run all indices within the windows size
                if token!= sentence_token_ids[pair] and sentence_token_ids[pair]!=unk_token_id: #check it's not the element itself and that are not unknown ids
                  data.append((token, sentence_token_ids[pair]))

    return data

"""These "unit tests" may help to 1) understand what it is expected from the function and 2) validate the function."""

assert create_skipgram_dataset([[1, 2, 3, 4, 5]], 0, 2) == [(3, 1), (3, 2), (3, 4), (3, 5)]
assert create_skipgram_dataset([[1, 2, 3, 4, 5, 6]], 0, 2) == [(3, 1), (3, 2), (3, 4), (3, 5), (4, 2), (4, 3), (4, 5), (4, 6)]
assert create_skipgram_dataset([[1, 2, 3, 0, 5, 6]], 0, 2) == [(3, 1), (3, 2), (3, 5)]
assert create_skipgram_dataset([[1, 0, 3, 4, 5]], 0, 2) == [(3, 1), (3, 4), (3, 5)]
assert create_skipgram_dataset([[1, 0, 3, 4, 5, 0, 7]], 0, 3) == [(4, 1), (4, 3), (4, 5), (4, 7)]

"""Now, let's prepare the actual datasets:"""

window_size = 2
skipgram_train = create_skipgram_dataset(train_token_ids, unk_token_id, window=window_size)
skipgram_valid = create_skipgram_dataset(valid_token_ids, unk_token_id, window=window_size)
skipgram_test = create_skipgram_dataset(test_token_ids, unk_token_id, window=window_size)

"""## Neural Network

Now we start with the Pytorch stuff. First, the model. Actually, it's the model together with the loss function:
"""

class SkipgramLoss(nn.Module):
    """
    Network that implements the basic Skipgram word2vec, without the negative
    sampling loss or hierarchical softmax.
    """

    def __init__(self, vocab_size, embedding_size):
        """
        Constructor.
        :param vocab_size Size of the vocabulary.
        :param embedding_size Size of the embedding vectors.
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(self, input, output):
        """
        :param input LongTensor with shape [batch_size, input_size]
        :param output LongTensor with shape [batch_size]
        """
        v_wi = self.embedding(input)    # [bsz, embsz]
        v_all = self.embedding.weight   # [vocab_size, embsz]
        
        # TODO: implement the computation of the logits of the softmax according
        #       to equation (2) in the article "Distributed Representations of
        #       Words and Phrases and their Compositionality".
        #       (http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality)
       
        logits=torch.mm(v_wi,torch.transpose(v_all,0,1))
        return self.cross_entropy(logits, output)
    
    def get_embedding_table(self):
        return self.embedding.weight

"""Now, let's define a function that trains the model for a whole epoch:"""

def train_epoch(train_data_loader, loss_function, optimizer, log_period, epoch):
  """
  Trains the given model for 1 epoch.
  :param train_data_loader Data loader of the training data. Batches must be
                           a tuple of input and output data.
  :param loss_function Loss function to train.
  :param optimizer Optimizer object to use.
  :param log_period Number of steps after which to print a trace.
  :param epoch Epoch to print in the traces.
  :return List of losses for every training step.
  """
  loss_function.train()

  num_total_data = len(train_data_loader.dataset)
  num_processed_data = 0

  losses = []
  for batch_ndx, batch in enumerate(train_data_loader):
      input, output = batch
      optimizer.zero_grad()
      loss = loss_function(input.cuda(), output.cuda())
      loss.backward()
      optimizer.step()
      loss_value = loss.item()
      losses.append(loss_value)
      num_processed_data += batch[0].size(0)

      if log_period and batch_ndx % log_period == 0:
          percentage = 100 * num_processed_data // num_total_data
          avg_loss = loss_value if batch_ndx == 0 else np.mean(losses[-log_period + 1:])
          msg = "epoch: {}, progress: {}%, avg.loss: {:.3f}"
          print(msg.format(epoch, percentage, avg_loss))
  
  return losses

"""Now, a function to evaluate our model in the validation data:"""

def eval_loss(data_loader, loss_function):
    """
    Computes the loss function over the given data in inference mode.
    :param data_loader Data loader over which to evaluate loss_function.
    :param loss_function Loss function to evaluate.
    :return Value (float) of the loss.
    """
    loss_function.eval()
    
    num_total_data = len(data_loader.dataset)

    with torch.no_grad():
        loss = 0.0
        for batch in data_loader:
            input, output = batch
            bsz = input.size(0)
            batch_loss = loss_function(input.cuda(), output.cuda())
            
            # we average over all batches
            loss += bsz * batch_loss.item() / num_total_data
    
    return loss

"""Finally, a function to create and train the embeddings for N epochs, evaluating the model every epoch:"""

def train_embedding(train_data,
                    valid_data,
                    vocab_size,
                    emb_size,
                    num_epochs,
                    batch_size,
                    log_period=None):
  """
  Trains embeddings with the given parameters.
  :param train_data Training data (list of tuples of input and output token IDs).
  :param valid_data Validation data (list of tuples of input and output token IDs).
  :param vocab_size Size of the vocabulary.
  :param emd_size Dimensionality of the embedding.
  :param num_epochs Number of epochs to train.
  :param batch_size Batch size to use.
  :param log_period Number of steps after which to print a trace.
  :return A tupe with the embedding tensor, the list of the training
          losses and the list of the validation losses.
  """

  # Load the whole training a validation data as pytorch tensors
  train_input = torch.from_numpy(np.array([input for (input, __) in train_data]))
  train_output = torch.from_numpy(np.array([output for (_, output) in train_data]))
  valid_input = torch.from_numpy(np.array([input for (input, _) in valid_data]))
  valid_output = torch.from_numpy(np.array([output for (_, output) in valid_data]))

  # Define data loaders for the training and validation data
  train_loader = DataLoader(TensorDataset(train_input, train_output),
                            batch_size=batch_size,
                            pin_memory=True)
  valid_loader = DataLoader(TensorDataset(valid_input, valid_output),
                            batch_size=batch_size,
                            pin_memory=True)

  # Create the model (+ loss function) and move it to the GPU
  loss_function = SkipgramLoss(vocab_size, emb_size).cuda()

  # Define the optimizer and LR scheduler
  optimizer = optim.Adam(lr=0.005, params=loss_function.parameters())
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)

  # Define lists to collect the training and validation losses in case
  # we want to plot them later.
  train_losses = []
  valid_losses = []

  # Iterate the requested number of epochs
  for epoch in range(num_epochs):
      # Train for one epoch
      train_losses.extend(train_epoch(train_loader,
                                      loss_function,
                                      optimizer,
                                      log_period,
                                      epoch))

      # Evaluate the model on the validation data
      valid_loss = eval_loss(valid_loader, loss_function)
      valid_losses.append(valid_loss)
      
      scheduler.step()

      if log_period:
          # Print traces every once in a while
          print("epoch: {}, validation loss: {:.3f}".format(epoch, valid_loss))
  
  # This is the embedding table that we have been training
  embedding = loss_function.get_embedding_table()

  return embedding, train_losses, valid_losses

"""And now we actually train the embeddings:"""

embedding, train_losses, valid_losses = train_embedding(skipgram_train,
                                                        skipgram_valid,
                                                        vocab_size=len(idx2token),
                                                        emb_size=64,
                                                        num_epochs=5,
                                                        batch_size=4000,
                                                        log_period=400)

"""---

## Explore the Embedded Vectors

First, let's define a function to find the list of K closes vectors:
"""

def find_closest_ids(embedding, vector, k):
    """
    Finds the closes K vectors to the given one among the given embedding matrix.
    :param embedding Embedding matrix.
    :param vector Vector to which we should find the closes ones in the matrix.
    :param k Number of closest elements to find.
    """
    similarity = F.cosine_similarity(embedding, vector.unsqueeze(0))
    index_sorted = torch.argsort(similarity).cpu().numpy()
    top_k = index_sorted[-k:]
    return list(reversed(top_k))

"""Let's find the words that are close to some target words:"""

words = ["minister", "France", "measures"]

for word in words:
    #TODO: print the closes 5 words to each of the words in the list
    emb_word = embedding[token2idx[word.lower()]]
    similar_token_ids = find_closest_ids(embedding, emb_word, 5)
    similar_words = [idx2token[sim] for sim in similar_token_ids]
    print("Similar words to {}: {}".format(word, similar_words))

"""The "geometry" of word embedded spaces has been widely studied, observing the  "parallelogram" formed by the vectors of two pairs of words with analogous relationships. You can read more about this in this recent ICML article: http://proceedings.mlr.press/v97/allen19a.html"""

# TODO: making use of the function find_closest_ids, try to find two pairs of
#       words that verify the parallelogram in embedding space and two pairs of
#       words that do not verify it.
'''
first word the original word
second the one we will substract
third the one that will be added
fourth the expected result
It prints the differnce vector between the expected result and the actual result
'''

words = [['man', 'doctor', 'nurse', 'woman'], ['man', 'programmer', 'housewife', 'woman'], ['man', 'doctor', 'woman', 'nurse'], ['father', 'man', 'woman', 'mother']]
for word in words:
  original = embedding[token2idx[word[0]]]
  substract = embedding[token2idx[word[1]]]
  add = embedding[token2idx[word[2]]]
  guess_result = embedding[token2idx[word[3]]]
  result = original - substract + add

  similar_token_ids = find_closest_ids(embedding, result, 5)
  similar_words = [idx2token[sim] for sim in similar_token_ids]
  print('guess: {}'.format(word[3]))
  print(" {} - {} + {} = {} \n".format(word[0], word[1], word[2], similar_words))

"""Nevertheless, be aware that many of the parallelogram examples in the literature (specially those that were considered to show gender bias) where due to an implementation detail in gensim, which explicitly avoided the returned vector to be the same as the original one. Read about this at https://twitter.com/rikvannoord/status/1132933236756238341


**TODO**: explain the misunderstanding that amplified our belief in the presence of biases in word embeddings.

<font color=blue> The main misunderstanding is that the word2vec has a gender bias, for example, if there is sought what a man doctor is for a woman the most probable result that you might get is a nurse. Despite showing an obvious bias, what hasn't been taken into account is that the queries in word2vec are the style of A:B :: C:D meaning that if you are querying a gender-neutral word such as "doctor" you cannot get the same word as a response.
If there is unrestricted the possibility of returning the same word in the widely famous analogy capability of word2vec, the problem that usually appears is that in the majority of cases the same word from the query is returned. Meaning, for example, that if there is searched what a man doctor is to a woman, almost certainly will appear 'doctor'. But the elimination of such a constraint quickly backfires when in a huge majority of cases the word2vec context returns the same result. For example, the analogy of a king man for a woman is no longer a queen.
The presence of biases sometimes have been overexposed, and probably a great analysis that could be done is to change the language used to a language with more gender declinations like catalan or spanish; where, for example, a female doctor is called 'doctora'.

---

This is not to say that our data, models or resulting word embeddings are bias free. On the contrary, there is **gender bias** in the three of them. Check the literature on gender bias in word embeddings to know more. You can start with these two seminal works:
* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings)
* [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061/)

---
"""