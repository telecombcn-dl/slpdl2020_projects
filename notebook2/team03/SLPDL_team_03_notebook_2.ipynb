{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLPDL_notebook2_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOn1cnCKQT6d",
        "colab_type": "text"
      },
      "source": [
        "# Instructions of this exercise\n",
        "\n",
        "This jupyter notebook is an exercise for you to understand the basics of LSTM language models.\n",
        "\n",
        "The different sections have comments that will guide you through the different stages of the processing.\n",
        "\n",
        "**You will find parts in this notebook that are missing. They are marked with a <font color=\"red\">\"TODO\"</font>, with an explanation about what you are supposed to write/code in that gap. Sometimes you may be requested to implement an algorithm or a piece of code that behaves in a specific way. Other times, you may be requested to explain what a specific piece of code does or why it is needed.** In some cases, the requests may contain a \"trap\", e.g. the thing you are requested to implement can't be done for whatever reason, or it may need an extra preprocessing step for it to be possible.\n",
        "\n",
        "If you have any doubt, do not hesitate to contact the instructor at noe.casas@upc.edu\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifhifRKlyotX",
        "colab_type": "text"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "In order to follow this notebook you need working knowledge on:\n",
        "\n",
        "* Python programming.\n",
        "* Pytorch basics.\n",
        "* Language modeling basics.\n",
        "* Unix command line basics (cat, head, tail, awk).\n",
        "\n",
        "If you lack any of them, please acquire the needed knowledge on the internet, where there is plenty of material about the three topics.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDyNSZSRXQ52",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Language Modeling is the task of predicting each token based on the previous ones. Formally, a language models the probability distribution of the token at position $t$, which is referred to as $y_t$, conditioning it on the previous tokens $y_1,\\ldots,y_{t-1}$, that is, an LM models $p\\left(y_t|  y_1, \\ldots, y_{t-1} \\right)$\n",
        "\n",
        "A classical approach for LMs is to use a recurrent unit, either an LSTM or a GRU. In this exercise you will be implementing an LSTM LM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6KYn0X5XjjJ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Installation of extra libraries\n",
        "\n",
        "The extra libraries we will use are:\n",
        "* **sacremoses**: the is a classical translation tool called \"Moses\" (https://github.com/moses-smt/mosesdecoder). Apart from translation tools, it offers a lot of different preprocessing scripts (written in the Perl programming language). These scripts are ubiquitous in the NLP world. Instead of using Moses scripts, we will use sacremoses, which is a reimplementation of the most useful Moses scripts in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9nO7Iu-FKup",
        "colab_type": "code",
        "trusted": true,
        "outputId": "24509ad7-7d73-4277-c339-ce111e5e9d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install -U https://github.com/alvations/sacremoses/archive/master.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.3)\n",
            "Collecting https://github.com/alvations/sacremoses/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/alvations/sacremoses/archive/master.zip\n",
            "\u001b[K     | 1.8MB 17.9MB/s\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.41) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.41) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.41) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.41) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.41) (4.38.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=884524 sha256=7983a25abad1aab4ded1224518b735da8f62ad108d87dc37b1daae1a229ccda6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_4_5o0o/wheels/a9/8b/db/e688b9d5eecd3518dd27712980c76599bcd8312ee220f4878e\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs5xmv2NdoRV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## General Imports\n",
        "\n",
        "Now we will import some components that will be used throughout the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TKrQWEltqVI",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeLR98bJuV8O",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Reproducibility\n",
        "\n",
        "We fix the random seed for reproducibility purposes. See this for details:\n",
        "https://pytorch.org/docs/stable/notes/randomness.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN_B25-ctw2t",
        "colab_type": "code",
        "trusted": true,
        "outputId": "756fe63f-e14b-4c06-a385-c57d9400237a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seed = 321\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "model_save_name = 'model1.pt'\n",
        "path = F\"/content/drive/My Drive/Telecos/Master/SLPDL/SLPDL_project/notebook2/{model_save_name}\" \n",
        "train = False # if True, train model. Otherwise, load model from drive."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm01kM-Td4El",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Download\n",
        "\n",
        "For this notebook we will use a dataset widely used in machine translation, called \"News Commentary\". We will be using some English subsets of News Commentary as training, validation and test data.\n",
        "\n",
        "First, we will download the dataset from the internet and extract some parts into files `train.en`, `valid.en` and `test.en`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ5lGJO9t55o",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "!wget --quiet 'http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz'\n",
        "!tar xzf training-parallel-nc-v12.tgz\n",
        "!mv training/news-commentary-v12.fr-en.en ./train.en\n",
        "!bash -c \"shuf --random-source=<(openssl enc -aes-256-ctr -pass pass:42 -nosalt </dev/zero 2>/dev/null)\" < training/news-commentary-v12.zh-en.en > shuffled.en\n",
        "!head -5000 < shuffled.en > valid.en\n",
        "!tail -5000 < shuffled.en > test.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLBlhh0ceaIX",
        "colab_type": "text"
      },
      "source": [
        "News commentary is text from news, where each line of the file is a separate sentence. Let's take a look at the number of sentences in each file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WurMWf7jvDKR",
        "colab_type": "code",
        "trusted": true,
        "outputId": "d1e0f558-a367-4002-e959-621aa64a64aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!wc -l train.en valid.en test.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  258432 train.en\n",
            "    5000 valid.en\n",
            "    5000 test.en\n",
            "  268432 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTftXikMemRy",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "In general, it is of utmost importance to understand the data you are working with, so let's have a look at the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO4icbYMEjgv",
        "colab_type": "code",
        "trusted": true,
        "outputId": "4afb8d44-bc58-406b-ba45-dbb41c432d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!head -5 train.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$10,000 Gold?\n",
            "SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.\n",
            "Lately, with gold prices up more than 300% over the last decade, it is harder than ever.\n",
            "Just last December, fellow economists Martin Feldstein and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment, sensibly pointing out gold’s risks.\n",
            "Wouldn’t you know it?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MJ0fZQzeyg5",
        "colab_type": "text"
      },
      "source": [
        "As a first preprocessing step, we will:\n",
        "* **normalize punctuation**: convert punctuation marks so that only the most standard ones are used. For instance, unicode curly quotes (“”) will be converted to straigh ones (\").\n",
        "* **tokenize**: words and punctuation will be separated with spaces, and contractions may be expanded. For instance, sentence \"No, he wasn't there.\" will be converted to \"No , he was n't there .\"\n",
        "* **lowercase**: turn all alphabetical characters into lowercase, so that the embeddings we obtain are independent from the original casing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeDdDO7WFH7X",
        "colab_type": "code",
        "trusted": true,
        "outputId": "05f36420-2a71-4309-fcb2-9032baa34b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!cat train.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > train.low.tok.en\n",
        "!cat valid.en | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > valid.low.tok.en\n",
        "!cat test.en  | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }' > test.low.tok.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 258961/258961 [00:30<00:00, 8440.60it/s]\n",
            "100% 258961/258961 [00:41<00:00, 6194.97it/s]\n",
            "100% 5000/5000 [00:00<00:00, 8619.84it/s]\n",
            "100% 5000/5000 [00:00<00:00, 5730.84it/s]\n",
            "100% 5004/5004 [00:00<00:00, 8618.74it/s]\n",
            "100% 5004/5004 [00:00<00:00, 5674.13it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9_1Y-HBuxF",
        "colab_type": "code",
        "outputId": "ea7d09a0-4b45-4a30-e9e3-b01f69d208da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!echo \"aren't\"  | sacremoses normalize -l en | sacremoses tokenize -l en | awk '{ print tolower($0) }'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 1/1 [00:00<00:00, 707.30it/s]\n",
            "100% 1/1 [00:00<00:00,  8.60it/s]\n",
            "aren &apos;t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAWv2wcgoY0",
        "colab_type": "text"
      },
      "source": [
        "Let's look how our data looks like after the described preprocessing steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVI139fcLZov",
        "colab_type": "code",
        "trusted": true,
        "outputId": "94977678-25d1-4b19-bf36-5736fcb9767e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "!head -10 train.low.tok.en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$ 10,000 gold ?\n",
            "san francisco - it has never been easy to have a rational conversation about the value of gold .\n",
            "lately , with gold prices up more than 300 % over the last decade , it is harder than ever .\n",
            "just last december , fellow economists martin feldstein and nouriel roubini each penned op-eds bravely questioning bullish market sentiment , sensibly pointing out gold &apos;s risks .\n",
            "wouldn &apos;t you know it ?\n",
            "since their articles appeared , the price of gold has moved up still further . gold prices even hit a record-high $ 1,300 recently .\n",
            "last december , many gold bugs were arguing that the price was inevitably headed for $ 2,000 .\n",
            "now , emboldened by continuing appreciation , some are suggesting that gold could be headed even higher than that .\n",
            "one successful gold investor recently explained to me that stock prices languished for a more than a decade before the dow jones index crossed the 1,000 mark in the early 1980 &apos;s .\n",
            "since then , the index has climbed above 10,000 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZOTS2VPjzxL",
        "colab_type": "text"
      },
      "source": [
        "Now we have our 3 final files: `train.low.tok.en`, `valid.low.tok.en` and `test.low.tok.en`.\n",
        "\n",
        "Note that, at each preprocessing step, we have been creating new files with extra suffixes indicating the type of preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yKXradWjvyi",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Text Loading\n",
        "\n",
        "We will load the text from our 3 files into python variables. The text in a file will be represented as a list of sentences, where each sentence is a list of words (strings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f31ITJwM40Sf",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def load_text_tokens(filename):\n",
        "    \"\"\"\n",
        "    Loads the text in the filename as a list of list of words.\n",
        "    :param filename Name of the file to load in memory.\n",
        "    :return List of list of strings with the contents of the file.\n",
        "    \"\"\"\n",
        "    text = []\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line = line.strip() # remove leading and training blanks\n",
        "\n",
        "            # split words at blanks (the text is already tokeniked)\n",
        "            line_tokens = line.split(' ')\n",
        "\n",
        "            if len(line_tokens) < 5:\n",
        "              continue  # ignore too short lines\n",
        "\n",
        "            text.append(line_tokens)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnSzCiFTUWV2",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train_tokens = load_text_tokens('train.low.tok.en')\n",
        "valid_tokens = load_text_tokens('valid.low.tok.en')\n",
        "test_tokens = load_text_tokens('test.low.tok.en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha5EHhoAlEfd",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary Extraction\n",
        "\n",
        "Given the huge amount of possible word in a language, when creating word embeddings, we need to constrain the set of supported words. This way, we will be having a list of words for which we will compute embeddings. This list is known as the **vocabulary**. The vocabulary is created by selecting the most frequent words in the training data.\n",
        "\n",
        "As the vocabulary is finite, there are some words it won't be able to represent. We will use a **special token** to represent these \"out of vocabulary\" words, normally referred to as the **&lt;UNK&gt;** token.\n",
        "\n",
        "We will also use another special token **&lt;EOS&gt;** to represent the ending of a sentente (EOS stands for end of sequence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDr3uo_YjrC9",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def extract_vocabulary(text_tokens, vocab_size):\n",
        "    from collections import Counter\n",
        "    unk_token = '<UNK>'\n",
        "    eos_token = '<EOS>'\n",
        "    special_tokens = [unk_token, eos_token]\n",
        "    counter = Counter(word for sentence in text_tokens for word in sentence)\n",
        "    most_common_counts = counter.most_common(vocab_size - len(special_tokens))\n",
        "    most_frequent_words = [word for (word, count) in most_common_counts]\n",
        "    idx2token = special_tokens + most_frequent_words\n",
        "    token2idx = {token: token_id for token_id, token in enumerate(idx2token)}\n",
        "    return unk_token, eos_token, idx2token, token2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXsDVis4jtWf",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE=30000\n",
        "unk_token, eos_token, idx2token, token2idx = extract_vocabulary(train_tokens, vocab_size=VOCAB_SIZE)\n",
        "unk_token_id = token2idx[unk_token]\n",
        "eos_token_id = token2idx[eos_token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2gLL3fGlKHx",
        "colab_type": "text"
      },
      "source": [
        "## Text Encoding\n",
        "\n",
        "To feed words to our neural network, we need to express them as numbers. This way, a word will be represented as the index it occupies in the vocabulary table. With this in mind, let's encoder the training, validation and test datasets:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxRk85UNkpk_",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "def encode_token_ids(text_tokens, vocabulary, unk_token_id, eos_token):\n",
        "    \"\"\"\n",
        "    Create a list with all the token IDs in the data. Note that all sentences\n",
        "    are concatenated one after the other.\n",
        "    :param text_tokens \n",
        "    \"\"\"\n",
        "    return [vocabulary.get(token, unk_token_id)\n",
        "            for sentence in text_tokens for token in sentence + [eos_token]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AUC2-Wiks7N",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "train_token_ids = torch.LongTensor(encode_token_ids(train_tokens, token2idx, unk_token_id, eos_token))\n",
        "valid_token_ids = torch.LongTensor(encode_token_ids(test_tokens, token2idx, unk_token_id, eos_token))\n",
        "test_token_ids = torch.LongTensor(encode_token_ids(test_tokens, token2idx, unk_token_id, eos_token))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOjoPmR9qMy1",
        "colab_type": "text"
      },
      "source": [
        "## Batch Preparation\n",
        "\n",
        "The way textual data is fed to a Language Model is not very self-evident (at least to me).\n",
        "\n",
        "First, some <u>background</u>:\n",
        "\n",
        "**How are images fed to image-processing neural networks?** Normally, we assume that the size of the images that will be used as input to a neural networl is constant, that is, all images are assumed to be the same size. This allows use to create mini-batches easily: if we have $n$ color images of size $w \\times h$, each image is represented by a tensor of shape [3, w, h] (the \"3\" is because a color image is normally represented with 3 channels: red, green and blue), so we just stack the $n$ images together into a tensor of shape `[n, 3, w, h]`. Therefore, because we have input data of the same size, creating batches is easy.\n",
        "\n",
        "**How are text sentences fed to neural machine translation (NMT) or text classification networks?** In these cases, each piece of input data is a sequence of tokens, where each token is an integer number, normally the index of the token in the vocabulary table. Each sentence can have a differnt length. In order to compose a mini-batch, we make all sentences in the batch artificially longer, to match the length of the longest sentence. In order to do that, we will each sentence to the right with a special \"padding token\". For instance, if we want to pad a sentence of length 5 to fit in a batch where the maximum sequence length is 8, we need to add 3 padding tokens to the right of the last token in the sentence.\n",
        "\n",
        "Now, **how are text sentences fed to language models?** It would be possible to do the same as with NMT/text classification networks, but that's not the usual way to do it. Why? Because that way we would not be using the context from the previous sentences to make the next token predictions. In order to be able to profit from previous sentences' information, we do as follows: we concatenate all sentences together in a single long super sequence (the boundary between sentences is normally marked with an &lt;EOS&gt; token) and then, we create the batches so that they include $n$ fragments of fixed-length text so that they are \"connected\" to the $n$ fragments from the previous batch that we gave to the network. This means that the piece of fixed-length text at position $k$ in a batch is the continuation of the sentence at position $k$ in the previous batch. To actually be able to use the context from the previous batch in the predictions of the current batch, the last hidden states of the previous batch are used as initial hidden states for the current batch. The propagation of the gradients between different batches is severed, though. This is called \"**truncated back-propagation through time**\" (T-BPTT). The following functions are in charge of preparing the mini-batches in the appropriate way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMQSwummqQi0",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def prepare_data(data, batch_size):\n",
        "    \"\"\"\n",
        "    Starting from sequential data, this function arranges the dataset into columns.\n",
        "    For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "    ┌ a g m s ┐\n",
        "    │ b h n t │\n",
        "    │ c i o u │\n",
        "    │ d j p v │\n",
        "    │ e k q w │\n",
        "    └ f l r x ┘.\n",
        "    These columns are treated as independent by the model, which means that the\n",
        "    dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "    batch processing.\n",
        "    :param data LongTensor with a vector with all the data to prepare.\n",
        "    :\n",
        "    \"\"\"\n",
        "    # Compute how we can divide the dataset into batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    \n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "\n",
        "    return data.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ9T9Ekdxow1",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "def get_batch_k(source, bptt, k):\n",
        "    \"\"\"\n",
        "    Subdivides the source data into chunks of length bptt (backprop through time).\n",
        "    If source is equal to the example output of the prepare_data function, with\n",
        "    a bptt-limit of 3, we'd get the following two Tensors for k = 0:\n",
        "    ┌ a g m s ┐ ┌ b h n t ┐\n",
        "    | b h n t | | c i o u |\n",
        "    └ c i o u ┘ └ d j p v ┘\n",
        "    Note that the subdivison of data is not done along the batch dimension\n",
        "    (i.e. dimension 1), since that was handled by the prepare_data function.\n",
        "    The chunks are along dimension 0, corresponding to the seq_len dimension in\n",
        "    the LSTM.\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - k)\n",
        "    input_start = k\n",
        "    input_end   = k + seq_len\n",
        "    input = source[input_start:input_end]\n",
        "    target = source[input_start + 1:input_end + 1].view(-1)\n",
        "    return input, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcDi8wM21D87",
        "colab_type": "text"
      },
      "source": [
        "# Training boilerplate\n",
        "\n",
        "Now we will define some functions to train and evaluate a language model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BFZdjHZACFn",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def train_epoch(model, criterion, optimizer, vocab_size, train_data, batch_size, bptt, log_interval, epoch):\n",
        "    losses = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    num_total_data = train_data.size(0)\n",
        "    num_batches = num_total_data // bptt\n",
        "\n",
        "    for batch, i in enumerate(range(0, num_total_data - 1, bptt)):\n",
        "\n",
        "        data, targets = get_batch_k(train_data, bptt, i)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # We detach the hidden state from how it was previously produced to\n",
        "        # avoid backpropagating all the way to start of the dataset.\n",
        "        hidden = tuple(h.detach() for h in hidden)\n",
        "\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if log_interval and batch % log_interval == 0:\n",
        "            percentage = 100 * batch // num_batches\n",
        "            msg = 'epoch {}, progress: {}% avg.loss: {}'\n",
        "            print(msg.format(epoch, percentage, losses[-1]))\n",
        "    \n",
        "    return losses\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlYDjNk6nnpu",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion, data, vocab_size, bptt, eval_batch_size):\n",
        "\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "\n",
        "    num_total_elems = data.size(0) * data.size(1)\n",
        "\n",
        "    loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "        for i in range(0, data.size(0) - 1, bptt):\n",
        "            input, target = get_batch_k(data, bptt, i)\n",
        "            batch_size = input.size(0) * input.size(1)\n",
        "            output, hidden = model(input, hidden)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            batch_loss = criterion(output_flat, target).item()\n",
        "            loss += batch_size * batch_loss\n",
        "    \n",
        "    return loss / num_total_elems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfeR6CgwmnMW",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def train_model(model, num_epochs, vocab_size, train_data, valid_data, bptt, train_batch_size, eval_batch_size, log_interval):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(lr=0.001, params=model.parameters())\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = train_epoch(model,\n",
        "                                   criterion,\n",
        "                                   optimizer,\n",
        "                                   vocab_size,\n",
        "                                   train_data,\n",
        "                                   train_batch_size,\n",
        "                                   bptt,\n",
        "                                   log_interval,\n",
        "                                   epoch)\n",
        "        train_losses.extend(epoch_losses)\n",
        "\n",
        "        valid_loss = evaluate(model, criterion, valid_data, vocab_size, bptt, eval_batch_size)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('epoch {}, validation loss: {:.3f}'.format(epoch, valid_loss))\n",
        "\n",
        "    return train_losses, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywVBbsHO6SOO",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "It's time for us to define the neural architecture of our language model.\n",
        "\n",
        "But first, let's think about how an LM is used: a language model predicts the next token based on the previous ones. In training time, this \"previous tokens\" are tokens from the training data, so we don't use the predicted token at position $t$, denoted $\\hat{y}_t$, as an input for the prediction of the next tokens $\\hat{y}_{t+1}, \\hat{y}_{t+2},\\ldots$. Instead, we always use as input the real tokens from the training data $y_i$. This is normally called \"**teacher forcing**\". Note that teacher forcing can only be used at training time, when we have real tokens. At inference time, teacher forcing cannot be used (see the Inference section below).\n",
        "\n",
        "Now that we know what teacher forcing is, we can proceed to implement our language model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFH8Q_SzmQ0",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout=.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.rnn_output_dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers,\n",
        "                           dropout=dropout if num_layers > 1 else None)\n",
        "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "        # initialize weights\n",
        "        init_range = 0.1\n",
        "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "        self.projection.bias.data.zero_()\n",
        "        self.projection.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        dims = (self.num_layers, batch_size, self.hidden_dim)\n",
        "        return (torch.zeros(*dims).cuda(),\n",
        "                torch.zeros(*dims).cuda())\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "        # TODO: implement the forward pass of the language model, making\n",
        "        #       use of the submodules defined in the constructor,\n",
        "        #       namely self.embedding, self.embedding_dropout,\n",
        "        #       self.rnn, self.rnn_output_dropout and self.projection.\n",
        "        #       You should use teacher forcing, which is described above.\n",
        "        \n",
        "        embeddings = self.embedding(input)\n",
        "        embeddings = self.embedding_dropout(embeddings)\n",
        "        lstm_out, hidden_state = self.rnn(embeddings, hidden_state)\n",
        "        logits = self.rnn_output_dropout(lstm_out)\n",
        "        logits = self.projection(logits)\n",
        "\n",
        "        return logits, hidden_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wraQA3Aw7Xon",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Let's prepare the training, validation and test data by means of function `prepare_data` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf6AFZIy3zLK",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 100\n",
        "VALID_BATCH_SIZE = 10\n",
        "BPTT = 10\n",
        "train_data = prepare_data(train_token_ids, batch_size=TRAIN_BATCH_SIZE)\n",
        "valid_data = prepare_data(valid_token_ids, batch_size=VALID_BATCH_SIZE)\n",
        "test_data  = prepare_data(test_token_ids,  batch_size=VALID_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHeJLTQt7kAQ",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Now we can actually create the LM and train it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DukJPmyE_ksX",
        "colab_type": "code",
        "outputId": "3237906c-cd13-4f1d-b0c7-1cb8c2f6c626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(idx2token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap6GWZr1QrN9",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# TODO: choose an appropriate combination of hyperparameter values (number\n",
        "#       of epochs, embedding dimensionality, hidden size, number of layers,\n",
        "#       batch size, etc).\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "LOG_INTERVAL = 400\n",
        "model = LanguageModel(len(idx2token), embed_dim=400, hidden_dim=512, num_layers=2).cuda()\n",
        "if train:\n",
        "  train_losses, valid_losses = train_model(model,\n",
        "                                          NUM_EPOCHS,\n",
        "                                          len(idx2token),\n",
        "                                          train_data,\n",
        "                                          valid_data,\n",
        "                                          BPTT,\n",
        "                                          TRAIN_BATCH_SIZE,\n",
        "                                          VALID_BATCH_SIZE,\n",
        "                                          LOG_INTERVAL)\n",
        "  torch.save(model.state_dict(), path)\n",
        "else:\n",
        "  model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE4P5eEc7-oC",
        "colab_type": "text"
      },
      "source": [
        "We can now plot the training and validation curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kztj620xNXf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if train:\n",
        "  plt.plot(train_losses, label='train')\n",
        "  valid_x_axis = [(1 + k) * len(train_losses)/len(valid_losses)\n",
        "                  for k in range(len(valid_losses))]\n",
        "  plt.plot(valid_x_axis, valid_losses, label='valid')\n",
        "  plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXA3xFlR73mg",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "We will now use our trained language model to actually generate text. Let's understand how that will happen. Remember from the beginning of the notebook that our LM actually is modeling the probability distribution of a token conditioned on the previous ones, right?: $p\\left(y_t|  y_1, \\ldots, y_{t-1} \\right)$ . So at each time step, we have a categorical probability distribution (i.e. the probability for the next token to be each of the possible tokens in our vocabulary). How do we obtain a token from a probability distribution over the token space? There are many ways but the most straightforward ones are:\n",
        "\n",
        "* Greedily: taking the most probable token (i.e. the token with highest probability, normally with `torch.argmax`).\n",
        "* Sampling: drawing a random sample from the categorical distribution, normally done with `torch.multinomial`.\n",
        "\n",
        "This way, we will predict the next token with one of the techniques above. Then, we will use this prediction as input for the next prediction, and so on. This is called \"**auto-regressive**\" inference.\n",
        "\n",
        "Nevertheless, we will not start from scratch. Instead, we will give the LM a piece of text (the \"context\") to be used as starting point, and then the LM should generate the following text.\n",
        "\n",
        "First of all, we need a function that performs over the same preprocessing over the context text that we performed for the training, validation and test data at the beginning of the notebook (normalization, tokenization and lowercasing), so that we can use it as input for our network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt_SrwjOSEK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sacremoses import MosesTokenizer, MosesDetokenizer, MosesPunctNormalizer\n",
        "\n",
        "normalizer = MosesPunctNormalizer()\n",
        "tokenizer = MosesTokenizer(lang='en')\n",
        "\n",
        "def text2token_ids(token2idx, unk_token_id, text):\n",
        "    # TODO: implement the same preprocessing as with the\n",
        "    #       train/valid/test datasets, using sacremoses programmatic API\n",
        "    #       (normalization, tokenization, lowercasing). You can\n",
        "    #       check how to use it at https://github.com/alvations/sacremoses/\n",
        "    text = normalizer.normalize(text)\n",
        "    text = tokenizer.tokenize(text)\n",
        "    text_token = [x.lower() for x in text]\n",
        "    text_token_ids = []\n",
        "    for token in text_token:\n",
        "      try:\n",
        "        text_token_ids.append(token2idx[token])\n",
        "      except KeyError:\n",
        "        text_token_ids.append(unk_token_id)\n",
        "    return text_token_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi2c4epjSRPG",
        "colab_type": "text"
      },
      "source": [
        "Now we are actually going to use the LM to generate text. \n",
        "\n",
        "\n",
        "For this, we will supply the context text to the language model. The output of the LSTM will be our first prediction. We will use it as the input for the next token prediction, and so on. We will also need to carry the hidden state generated from the previous prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txc961YReeKK",
        "colab_type": "text"
      },
      "source": [
        "We will start by implementing greedy inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtI4sYm4BBle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_greedy(model, token2idx, idx2token, unk_token_id, eos_token_id, prefix_text, prediction_length):\n",
        "  # TODO: implement greedy inference\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predicted_text = prefix_text\n",
        "    hidden = model.init_hidden(1)\n",
        "    previous = prefix_text\n",
        "    \n",
        "    for _ in range(prediction_length):\n",
        "\n",
        "      input_idx = torch.LongTensor(text2token_ids(token2idx, unk_token_id, previous))\n",
        "      input = input_idx.view(-1,1).cuda()\n",
        "      out, hidden = model(input, hidden)\n",
        "      \n",
        "      predicted_token_id = out[out.shape[0]-1].argmax().item()\n",
        "      predicted_token = idx2token[predicted_token_id]\n",
        "      predicted_text = ' '.join([predicted_text, predicted_token])\n",
        "      if predicted_token_id == eos_token_id:\n",
        "        break \n",
        "      previous = predicted_token\n",
        "  \n",
        "  print(predicted_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNkqc16uexqt",
        "colab_type": "text"
      },
      "source": [
        "Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7mXX3wexBi",
        "colab_type": "code",
        "outputId": "72023b6b-5490-4a6a-c7a0-9c2443a22cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "inference_greedy(model, token2idx, idx2token, unk_token_id, eos_token_id, \"Hospitals have been\", 20)\n",
        "inference_greedy(model, token2idx, idx2token, unk_token_id, eos_token_id, \"The Prime Minister declared\", 20)\n",
        "inference_greedy(model, token2idx, idx2token, unk_token_id, eos_token_id, \"The economy in\", 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hospitals have been <UNK> , and the <UNK> , which has been the most important source of the <UNK> . <EOS>\n",
            "The Prime Minister declared that the us would be able to pay the same way to the us . <EOS>\n",
            "The economy in the world &apos;s &amp; p 500 , the <UNK> , <UNK> , <UNK> , <UNK> , <UNK> <UNK> <UNK> <UNK>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99oE7LVevCs",
        "colab_type": "text"
      },
      "source": [
        "Now we will implement random sampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmNhhNHobegZ",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def inference_sampling(model, token2idx, idx2token, unk_token_id, eos_token_id, prefix_text, prediction_length):\n",
        "    # TODO: implement inference with sampling from the\n",
        "    #       categorical (multinomial) probability distribution.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      predicted_text = prefix_text\n",
        "      hidden = model.init_hidden(1)\n",
        "      previous = prefix_text\n",
        "    \n",
        "      for _ in range(prediction_length):\n",
        "        input_idx = torch.LongTensor(text2token_ids(token2idx, unk_token_id, previous))\n",
        "        input_idx = input_idx.view(-1,1).cuda()\n",
        "        out, hidden = model(input_idx.cuda(), hidden)\n",
        "        soft_values = torch.nn.functional.softmax(out[input_idx.size(0)-1],dim=0)\n",
        "        predicted_token_id = torch.multinomial(soft_values, 1)\n",
        "        predicted_token = idx2token[predicted_token_id]\n",
        "        predicted_text = ' '.join([predicted_text, predicted_token])\n",
        "        \n",
        "        if predicted_token_id == eos_token_id:\n",
        "          break \n",
        "        previous = predicted_token\n",
        "  \n",
        "    print(predicted_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHFmGlxde7ji",
        "colab_type": "text"
      },
      "source": [
        "Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FLy_SoOnx9e",
        "colab_type": "code",
        "trusted": true,
        "outputId": "8b15ca79-42fc-48c1-86a7-c9899596e4f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "inference_sampling(model, token2idx, idx2token, unk_token_id, eos_token_id, \"Hospitals in new york\", 20)\n",
        "inference_sampling(model, token2idx, idx2token, unk_token_id, eos_token_id, \"The Prime Minister declared\", 20)\n",
        "inference_sampling(model, token2idx, idx2token, unk_token_id, eos_token_id, \"The economy in\", 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hospitals in new york casini borrows low-tech therefore objected rivaled cost-effective impaired scares bluff tingkat intolerable midterm deposits weary ores points vanguard dampened anak\n",
            "The Prime Minister declared facilitate underperforming quotes complete стран hungary czechoslovakia cognitive mengalami kepala referees estrangement wasteland actively түседі adverse forbids skyrocketing symmetry coalition\n",
            "The economy in pemberontak forever enron megalomaniacal smallpox on-the-job activated combative honoring highly salafi tantrum well-governed emptied neville unasur berjumlah barat advent wellington\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcRWs1tOMg2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}